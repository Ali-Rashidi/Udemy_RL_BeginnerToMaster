{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95e5d79",
   "metadata": {},
   "source": [
    "# DQN with Pytorch_Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4580930",
   "metadata": {},
   "source": [
    "Note: Use pytorch_lighning==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8960d718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num gpus : 1\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"num gpus : {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb90c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Deep Q Network:\n",
    "class DQN(nn.Module):\n",
    "    def __init__ (self , hidden_size , obs_size , action_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Linear(obs_size , hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size , hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size , action_size),\n",
    "        )\n",
    "    def forward(self , x):\n",
    "        return self.net(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58800a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Policy: state -> action or action_probs\n",
    "def epsilon_greedy(state , env , net , epsilon=0):\n",
    "    if np.random.random() < epsilon :\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    else:\n",
    "        state = torch.tensor([state]).to(device)\n",
    "        q_values = net(state)\n",
    "        _ , action = torch.max(q_values , dim=1) # returns (value , idx)\n",
    "        action = int(action.item())\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a59d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Replay Buffer:\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self , capacity):\n",
    "        self.buffer = deque(maxlen=capacity) #    it's like a list but manages its contents automaticly\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self , batch_size):\n",
    "        return random.sample(self.buffer , batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b9a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \n",
    "    def __init__ (self , buffer , sample_size = 200):\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for experience in self.buffer.sample(self.sample_size):\n",
    "            yield experience # returns by request of pytorch\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8ae4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Environment\n",
    "def create_environment(name):\n",
    "    env = gym.make(name)\n",
    "    env = TimeLimit(env , max_episode_steps = 400)  #terminates after 400 steps\n",
    "    env = RecordVideo(env , video_folder = './videos' , episode_trigger=lambda x: x%50==0 )\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e4a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\site-packages\\gym\\wrappers\\record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at C:\\Users\\Ali\\Documents\\RLwithPhil\\code\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00352249,  1.4044876 ,  0.35678267, -0.28589943, -0.00407496,\n",
       "       -0.08081645,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = create_environment('LunarLander-v2')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d6d2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1812e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_environment('LunarLander-v2')\n",
    "# for e in range(10):\n",
    "#     done = False\n",
    "#     env.reset()\n",
    "#     while not done:\n",
    "#         action = env.action_space.sample()\n",
    "#         _ , _ , done , _ = env.step(action)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99273df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning(LightningModule):\n",
    "    \n",
    "    # intialize\n",
    "    def __init__(self , env_name , policy=epsilon_greedy , capacity=100_000 , batch_size=1024 , lr = 0.001 ,\n",
    "                 hidden_size=128 , gamma=0.99, loss_fn = F.smooth_l1_loss , optim = AdamW ,\n",
    "                eps_start = 1.0 , eps_end = 0.15 , eps_last_episode=100 , samples_per_epoch=10_000 ,\n",
    "                sync_rate=10):\n",
    "        super().__init__()\n",
    "        self.env = create_environment(env_name)\n",
    "        obs_size=self.env.observation_space.shape[0]\n",
    "        action_size = self.env.action_space.n\n",
    "        self.q_net=DQN(hidden_size , obs_size , action_size)\n",
    "        self.target_q_net=copy.deepcopy(self.q_net)\n",
    "        self.policy = policy\n",
    "        self.buffer = ReplayBuffer(capacity=capacity)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        while len(self.buffer)  < self.hparams.samples_per_epoch:\n",
    "            self.play_episode(epsilon=self.hparams.eps_start)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def play_episode(self ,policy=None ,  epsilon =0):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done :\n",
    "            \n",
    "            if policy:\n",
    "                action = policy(state , self.env , self.q_net , epsilon = epsilon)\n",
    "            else:\n",
    "                action = self.env.action_space.sample()\n",
    "            next_state , reward , done , _ = self.env.step(action)\n",
    "            exp = (state , action , reward , done , next_state)\n",
    "            self.buffer.append(exp)\n",
    "            state = next_state\n",
    "            \n",
    "        \n",
    "        \n",
    "    # forward\n",
    "    def forward(self , x):\n",
    "        return self.q_net(x)\n",
    "    \n",
    "    \n",
    "    # configure optimizers\n",
    "    def configure_optimizers(self):\n",
    "        q_net_optimizer = self.hparams.optim(self.q_net.parameters() , lr = self.hparams.lr)\n",
    "        return [q_net_optimizer]\n",
    "    \n",
    "    \n",
    "    # create dataloader\n",
    "    def train_dataloader(self):\n",
    "        dataset = RLDataset(self.buffer , self.hparams.samples_per_epoch)\n",
    "        dataloader = DataLoader(dataset=dataset ,batch_size=self.hparams.batch_size )\n",
    "        return dataloader\n",
    "    \n",
    "    \n",
    "    # training step\n",
    "    def training_step(self , batch , batch_idx):\n",
    "        states , actions , rewards , dones , next_states = batch\n",
    "        actions = actions.unsqueeze(1)\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        dones = dones.unsqueeze(1)\n",
    "        state_action_values = self.q_net(states).gather(1,actions)\n",
    "        next_action_values , _ = self.target_q_net(next_states).max(dim=1 , keepdim=True)\n",
    "        expected_state_action_values = rewards + self.hparams.gamma * next_action_values * (torch.logical_not(dones))\n",
    "        loss = self.hparams.loss_fn(state_action_values , expected_state_action_values )\n",
    "        self.log('episode/Q-error' , loss)\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    \n",
    "    # training epoch end\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        epsilon = max(self.hparams.eps_end , self.hparams.eps_start - self.current_epoch/self.hparams.eps_last_episode)\n",
    "        self.play_episode(policy=self.policy , epsilon=epsilon)\n",
    "        self.log('episode/Return' , self.env.return_queue[-1])\n",
    "        \n",
    "        if self.current_epoch % self.hparams.sync_rate == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69283a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'lightning_logs/': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'videos/': No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13104), started 0:00:48 ago. (Use '!kill 13104' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bee9b4a9d6435350\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bee9b4a9d6435350\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!rm -r lightning_logs/\n",
    "#!rm -r videos/\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea8ca17c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: C:\\Users\\Ali\\Documents\\RLwithPhil\\code\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "0 | q_net        | DQN  | 18.2 K\n",
      "1 | target_q_net | DQN  | 18.2 K\n",
      "--------------------------------------\n",
      "36.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "36.4 K    Total params\n",
      "0.145     Total estimated model params size (MB)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f121cf662df424a8530f1342b667b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x0000022E489133A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 185, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 101, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 332, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 858, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\_weakrefset.py\", line 114, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x0000022E5AC489F0; to 'Win32Window' at 0x0000022E427B29D0>\n",
      "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_8256\\2993271317.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  state = torch.tensor([state]).to(device)\n"
     ]
    }
   ],
   "source": [
    "algo = DeepQLearning('LunarLander-v2')\n",
    "trainer = Trainer ( gpus = num_gpus , max_epochs = 10_000 , callbacks=[EarlyStopping(monitor = 'episode/Return' , mode='max',patience = 500)] )\n",
    "trainer.fit(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8215a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
