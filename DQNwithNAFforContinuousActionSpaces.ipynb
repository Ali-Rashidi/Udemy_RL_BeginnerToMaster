{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b4a1d0",
   "metadata": {},
   "source": [
    "# DQN with NAF (Normalized Advantage Function) for Continuous Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6e0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "from gym.wrappers import RecordVideo, RecordEpisodeStatistics , TimeLimit\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61bee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Q Network :\n",
    "class NafDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self , hidden_size , obs_size , action_dims , max_action) : # max_value scale our action\n",
    "        super().__init__()\n",
    "        self.action_dims = action_dims\n",
    "        self.max_action = torch.from_numpy(max_action).to(device)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size , hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size , hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear_mu = nn.Linear(hidden_size , action_dims)\n",
    "        \n",
    "        self.linear_value = nn.Linear(hidden_size , 1)\n",
    "        \n",
    "        self.linear_matrix = nn.Linear(hidden_size , int(action_dims * (action_dims+1)/2)) \n",
    "        # A trick to make the computation of P more efficent and to ensure its PD\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Mu : Compute the action with the highest Q-value\n",
    "    @torch.no_grad()\n",
    "    def mu(self , x ):\n",
    "        x = self.net(x)\n",
    "        x = self.linear_mu(x)\n",
    "        x = torch.tanh(x) * self.max_action # -> to keep the outputs in the range of action space\n",
    "        return x\n",
    "    \n",
    "    # Value : Compute the value of each state\n",
    "    @torch.no_grad()\n",
    "    def value(self , x):\n",
    "        x = self.net(x)\n",
    "        x = self.linear_value(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    # Forward : Compute Q as a function of mu and value\n",
    "    def forward(self , x , a):\n",
    "        x = self.net(x)\n",
    "        mu = torch.tanh(self.linear_mu(x)) * self.max_action\n",
    "        value = self.linear_value(x)\n",
    "        \n",
    "        #P(x)\n",
    "        matrix = torch.tanh(self.linear_matrix(x))\n",
    "        L = torch.zeros((x.shape[0] , self.action_dims , self.action_dims)).to(device) # batch_size * action_dims * action_dim\n",
    "        trill_indices = torch.tril_indices(row=self.action_dims , col = self.action_dims).to(device)\n",
    "        L[ : , trill_indices[0] , trill_indices[1]] = matrix\n",
    "        L.diagonal(dim1=1 , dim2=2).exp_() # to ensure P is PD\n",
    "        P = L * L.transpose(2,1)\n",
    "        \n",
    "        u_mu = (a-mu).unsqueeze(dim=1)\n",
    "        u_mu_t = u_mu.transpose(1,2)\n",
    "        \n",
    "        adv = - 0.5 * u_mu @ P @ u_mu_t\n",
    "        adv = adv.squeeze(dim=-1)\n",
    "        \n",
    "        Q = value + adv\n",
    "        \n",
    "        return Q\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05187b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Policy\n",
    "def noisy_policy(state , env , net , epsilon=0.0):\n",
    "    state = torch.tensor([state]).to(device)\n",
    "    amin = torch.from_numpy(env.action_space.low).to(device)\n",
    "    amax = torch.from_numpy(env.action_space.high).to(device)\n",
    "    mu = net.mu(state) # estimating best action\n",
    "    mu = mu + torch.normal(0,epsilon , mu.size() , device=device) # for exploration :\n",
    "    #at first epsilon is high however as the agent gets better epsilon gets smaller as well\n",
    "    action = mu.clamp(amin , amax)\n",
    "    action = action.squeeze().cpu().numpy()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db60fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Replay Buffer:\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self , capacity):\n",
    "        self.buffer = deque(maxlen=capacity) #    it's like a list but manages its contents automaticly\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self , batch_size):\n",
    "        return random.sample(self.buffer , batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79d60705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \n",
    "    def __init__ (self , buffer , sample_size = 200):\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for experience in self.buffer.sample(self.sample_size):\n",
    "            yield experience # returns by request of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b44534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatActionWrapper(gym.Wrapper): # every time we select an action we apply it multiple times before moving on\n",
    "                                        # to the next state and action. This makes our choice of actions more consistant &\n",
    "                                        # it also simplifies learning process because the agent has to choose an action less often\n",
    "    def __init__(self , env , n): \n",
    "        super().__init__(env)\n",
    "        self.env=env\n",
    "        self.n = n\n",
    "    \n",
    "    def step(self , action):\n",
    "        done = False\n",
    "        tatal_reward = 0\n",
    "        \n",
    "        for _ in range(self.n):\n",
    "            next_state , reward , done , info = self.env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return next_state , reward , done , info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e267700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Environment\n",
    "def create_environment(name):\n",
    "    env = gym.make(name)\n",
    "    env = TimeLimit(env , max_episode_steps = 400)  #terminates after 400 steps\n",
    "    env = RecordVideo(env , video_folder = './videos' , episode_trigger=lambda x: x%50==0 )\n",
    "    env = RepeatActionWrapper(env , n = 8)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123876de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_average(net , target_net , tau=0.01):\n",
    "    for qp,tp in zip(net.parameters() , target_net.parameters()):\n",
    "        tp.data.copy_(tau * qp.data + (1-tau)*tp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1587f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAFDeepQLearning(LightningModule):\n",
    "    \n",
    "    def __init__(self , env_name , policy = noisy_policy , capacity = 100_000 , batch_size=256 , lr = 1e-4 ,\n",
    "                 hidden_size=512 , gamma = 0.99 , loss_fn = F.smooth_l1_loss , optim = AdamW , eps_start = 2.0,\n",
    "                 eps_end=0.1 , eps_last_episode=1000 , samples_per_epoch=1000 , tau=0.01 ):\n",
    "        super().__init__()\n",
    "        self.env = create_environment(env_name)\n",
    "        obs_size = self.env.observation_space.shape[0]\n",
    "        action_dims = self.env.action_space.shape[0]\n",
    "        max_action = self.env.action_space.high\n",
    "        \n",
    "        self.q_net= NafDQN(hidden_size , obs_size , action_dims , max_action).to(device)\n",
    "        self.target_q_net = copy.deepcopy(self.q_net)\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.buffer = ReplayBuffer(capacity=capacity)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        while len(self.buffer)  < self.hparams.samples_per_epoch:\n",
    "            self.play_episode(epsilon=self.hparams.eps_start)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def play_episode(self ,policy=None ,  epsilon =0):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done :\n",
    "            \n",
    "            if policy:\n",
    "                action = policy(state , self.env , self.q_net , epsilon = epsilon)\n",
    "            else:\n",
    "                action = self.env.action_space.sample()\n",
    "            next_state , reward , done , _ = self.env.step(action)\n",
    "            exp = (state , action , reward , done , next_state)\n",
    "            self.buffer.append(exp)\n",
    "            state = next_state\n",
    "            \n",
    "     # forward\n",
    "    def forward(self , x):\n",
    "        output = self.q_net.mu(x)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    # configure optimizers\n",
    "    def configure_optimizers(self):\n",
    "        q_net_optimizer = self.hparams.optim(self.q_net.parameters() , lr = self.hparams.lr)\n",
    "        return [q_net_optimizer]\n",
    "    \n",
    "    \n",
    "    # create dataloader\n",
    "    def train_dataloader(self):\n",
    "        dataset = RLDataset(self.buffer , self.hparams.samples_per_epoch)\n",
    "        dataloader = DataLoader(dataset=dataset ,batch_size=self.hparams.batch_size )\n",
    "        return dataloader\n",
    "    \n",
    "    \n",
    "    # training step\n",
    "    def training_step(self , batch , batch_idx):\n",
    "        states , actions , rewards , dones , next_states = batch\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        dones = dones.unsqueeze(1)\n",
    "        \n",
    "        action_values = self.q_net(states,actions)\n",
    "        next_state_values = self.target_q_net.value(next_states)\n",
    "        next_state_values[dones]=0.0\n",
    "        \n",
    "        \n",
    "        target = rewards + self.hparams.gamma * next_state_values\n",
    "        \n",
    "        loss = self.hparams.loss_fn(action_values , target)\n",
    "        self.log('episode/Q-loss' , loss)\n",
    "        return loss\n",
    "    \n",
    "        # training epoch end\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        epsilon = max(self.hparams.eps_end , self.hparams.eps_start - self.current_epoch/self.hparams.eps_last_episode)\n",
    "        self.play_episode(policy=self.policy , epsilon=epsilon)\n",
    "        polyak_average(self.q_net , self.target_q_net , tau = self.hparams.tau)\n",
    "        \n",
    "        self.log('episode/Return' , self.env.return_queue[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3493fcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-390e670b447b060c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-390e670b447b060c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!rm -r lightning_logs/\n",
    "#!rm -r videos/\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/version_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b1f246c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: C:\\Users\\Ali\\Documents\\RLwithPhil\\code\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type   | Params\n",
      "----------------------------------------\n",
      "0 | q_net        | NafDQN | 270 K \n",
      "1 | target_q_net | NafDQN | 270 K \n",
      "----------------------------------------\n",
      "540 K     Trainable params\n",
      "0         Non-trainable params\n",
      "540 K     Total params\n",
      "2.163     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f4708b89ad42b0b003aaf5da11ebbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x000002AAE6A010D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 185, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 101, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 332, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 858, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\vrep\\lib\\_weakrefset.py\", line 114, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x000002AB262A9B80; to 'Win32Window' at 0x000002AAD3956550>\n"
     ]
    }
   ],
   "source": [
    "algo = NAFDeepQLearning('LunarLanderContinuous-v2' , lr=1e-3)\n",
    "trainer = Trainer ( gpus = num_gpus , max_epochs = 3_000 )\n",
    "trainer.fit(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31cb1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
